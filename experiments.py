"""experiments.py: Experiments on the truncated and censored data."""

__author__      = "Ba-Hien TRAN"
__email__       = "bahientranvn@gmail.com"


import sys
import warnings
import random

import numpy as np
import scipy as sp
import matplotlib as mpl
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn import mixture

from censored_data_utils import censor_and_truncate_data
from visualization import plot_gmm_data
from parameters_initialization import init_kmeans
from standard_em import perform_standard_em
from truncated_em import perform_truncated_em
from gmm_dataset import generate_gmm_data, build_GMM_model,\
     estimate_kl_divergence_gmm, reorder_gmm_compoments

if not sys.warnoptions:
    warnings.simplefilter("ignore")


def perform_bivariate_3_gaussians_exp(N, pp, mu_1, mu_2, mu_3,
                                      sigma_1, sigma_2, sigma_3, 
                                      truncation_bounds, censoring_bounds,
                                      max_iteration=50, seed=100):
    """Perform experiment on bivariate dataset generated from 3 Gaussians.

    Args:
        N (int): 
        pp (1D numpy array): The mixing weights.
        mu_1 (1D numpy array): The mean corresponding of the Gaussian 1.
        mu_2 (1D numpy array): The mean corresponding of the Gaussian 2.
        mu_3 (1D numpy array): The mean corresponding of the Gaussian 3.
        sigma_1 (2D numpy array): The covariance corresponding of the Gaussian 1.
        sigma_2 (2D numpy array): The covariance corresponding of the Gaussian 2.
        sigma_3 (2D numpy array): The covariance corresponding of the Gaussian 3.
        truncation_bounds (1D numpy array): The truncation bounds applied on the
            dataset.
        censoring_bounds (1D numpy array): The censoring bounds applied on the
            dataset.
        max_iteration (int): The maximum number of iterations.
        seed (int): The random seed for reproducibility.
    """
    # Fix the random state
    random.seed(seed)
    np.random.seed(seed)

    # Stack and reorder the means and covariance matrices into unified matrices
    mu = np.stack([mu_1, mu_2, mu_3], axis=0)
    sigma = np.stack([sigma_1, sigma_2, sigma_3], axis=2)
    pp, mu, sigma = reorder_gmm_compoments(pp, mu, sigma)
    K = mu.shape[0]

    # Generate GMM data
    print("Step #1: Generating a Gaussian-Mixture-Model dataset")
    print("True parameters:")
    print("pp: \n{}\n".format(pp))
    print("mu: \n{}\n".format(mu))
    print("sigma: \n{}\n".format(sigma.T))
    y = generate_gmm_data(pp, mu, sigma, N)

    # Plot the GMM data
    plt.figure()
    ax = plot_gmm_data(y, mu, sigma, point_color='black')
    plt.title("The Original Data Generated by Three Gaussian Components")
    print("\n" + "*"*80)

    # Perform censoring and truncation on the original data
    print("Step #2: Censoring and truncating the data")
    x = censor_and_truncate_data(y)

    # Plot the censored and truncated data
    plt.figure()
    ax = plot_gmm_data(y, mu, sigma, point_color='red')
    plt.title("Truncated and Censored Data")
    print("\n" + "*"*80)

    # Init parameters using K-means
    print("Step #3: Initializing parameters using K-means")
    par = init_kmeans(x, K)
    print("\n" + "*"*80)

    # Estimating parameters using truncated and censored EM
    print("Step #4: Estimating parameters using truncated and censored EM")
    tc_em_results = perform_truncated_em(x, K, 
                                         truncation_bounds, censoring_bounds, 
                                         par['pp'], par['mu'], par['sigma'], 
                                         max_iteration)

    print("Estimated parameters by standard EM:")
    print("pp: \n{}\n".format(tc_em_results['pp']))
    print("mu: \n{}\n".format(tc_em_results['mu']))
    print("sigma: \n{}\n".format(tc_em_results['sigma'].T))
    
    plt.figure()
    plt.plot(range(len(tc_em_results['ll_hist'])), tc_em_results['ll_hist'])
    plt.title("Learning Curve of the Truncated and Censored EM")
    plt.xlabel("Iteration")
    plt.ylabel("Log-likelihood")

    plt.figure()
    ax = plot_gmm_data(x, tc_em_results['mu'], tc_em_results['sigma'])
    plt.title("Truncated and Censored EM")
    print("\n" + "*"*80)

    print("Step #5: Estimating parameters using standard EM")
    std_em_results = perform_standard_em(x, K, seed)

    print("Estimated parameters by standard EM:")
    print("pp: \n{}\n".format(std_em_results['pp']))
    print("mu: \n{}\n".format(std_em_results['mu']))
    print("sigma: \n{}\n".format(std_em_results['sigma'].T))

    plt.figure()
    ax = plot_gmm_data(x, std_em_results['mu'], std_em_results['sigma'])
    plt.title("Standard EM")
    print("\n" + "*"*80)

    # Evaluate the KL-Divergence between true distribution and estimated 
    # distributions
    print("Step #6: Evaluating the estimated parameters")
    pp, mu, sigma = reorder_gmm_compoments(pp, mu, sigma)
    tc_em_results['pp'], tc_em_results['mu'], tc_em_results['sigma'] =\
        reorder_gmm_compoments(tc_em_results['pp'], tc_em_results['mu'],
                               tc_em_results['sigma'])
    std_em_results['pp'], std_em_results['mu'], std_em_results['sigma'] =\
        reorder_gmm_compoments(std_em_results['pp'], std_em_results['mu'],
                               std_em_results['sigma'])
                               
    true_gmm = build_GMM_model(pp, mu, sigma, seed)
    tc_gmm = build_GMM_model(tc_em_results['pp'], tc_em_results['mu'],
                             tc_em_results['sigma'], seed)
    std_gmm = build_GMM_model(std_em_results['pp'], std_em_results['mu'],
                              std_em_results['sigma'], seed)
                              
    tc_kl = estimate_kl_divergence_gmm(true_gmm, tc_gmm)
    std_kl = estimate_kl_divergence_gmm(true_gmm, std_gmm)    

    print("\t* KL-Divergence corresponding to truncated and censored EM: {}").\
        format(tc_kl)
    print("\t* KL-Divergence corresponding to standard EM: {}").\
        format(std_kl)
    print("\n" + "*"*80)

    # Show the plots
    print("Step #7: Showing the plots")
    plt.show()
